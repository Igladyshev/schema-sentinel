{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DuckDB Alternative: High-Performance Middle Layer for YAML Shredder\n",
    "\n",
    "This notebook demonstrates using **DuckDB** instead of SQLite as the middle layer for:\n",
    "- Loading YAML/JSON into relational tables\n",
    "- Transforming and querying data\n",
    "- Generating SQL DDL for multiple database systems (Snowflake, PostgreSQL, MySQL, SQLite)\n",
    "\n",
    "## Why DuckDB?\n",
    "- **Faster**: In-memory OLAP engine\n",
    "- **Better SQL Support**: Full ANSI SQL + extensions\n",
    "- **Rich Type System**: Better type inference\n",
    "- **JSON Native**: Excellent JSON/nested data handling\n",
    "- **Flexible**: In-memory or file-based\n",
    "- **No dependencies**: Single dependency with no C extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Installation\n",
    "\n",
    "First, ensure the yaml_shredder package with DuckDB support is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "from yaml_shredder import (\n",
    "    TableGenerator,\n",
    "    DDLGenerator,\n",
    "    StructureAnalyzer,\n",
    "    DuckDBLoader,\n",
    "    load_to_duckdb,\n",
    ")\n",
    "\n",
    "print(\"âœ“ All imports successful\")\n",
    "print(f\"âœ“ DuckDBLoader available: {DuckDBLoader}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Section 2: Create Sample YAML Data\n",
    "\n",
    "Create a complex YAML structure to demonstrate DuckDB's advantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary working directory\n",
    "TEMP_DIR = Path(tempfile.gettempdir()) / \"duckdb_shredder_demo\"\n",
    "if TEMP_DIR.exists():\n",
    "    shutil.rmtree(TEMP_DIR)\n",
    "TEMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sample YAML with complex nested structure\n",
    "sample_config = {\n",
    "    \"organization\": \"TechCorp\",\n",
    "    \"region\": \"us-east\",\n",
    "    \"clusters\": [\n",
    "        {\n",
    "            \"name\": \"prod-cluster\",\n",
    "            \"environment\": \"production\",\n",
    "            \"nodes\": [\n",
    "                {\"id\": \"node-1\", \"cpu\": \"16\", \"memory\": \"64GB\"},\n",
    "                {\"id\": \"node-2\", \"cpu\": \"16\", \"memory\": \"64GB\"},\n",
    "            ],\n",
    "            \"networking\": {\n",
    "                \"vpc\": \"vpc-prod\",\n",
    "                \"subnets\": [\"subnet-1\", \"subnet-2\"],\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"dev-cluster\",\n",
    "            \"environment\": \"development\",\n",
    "            \"nodes\": [\n",
    "                {\"id\": \"node-3\", \"cpu\": \"4\", \"memory\": \"16GB\"},\n",
    "            ],\n",
    "            \"networking\": {\n",
    "                \"vpc\": \"vpc-dev\",\n",
    "                \"subnets\": [\"subnet-3\"],\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "    \"services\": {\n",
    "        \"api\": {\"replicas\": 3, \"cpu\": \"2\", \"memory\": \"4GB\"},\n",
    "        \"database\": {\"replicas\": 1, \"cpu\": \"8\", \"memory\": \"32GB\"},\n",
    "        \"cache\": {\"replicas\": 2, \"cpu\": \"1\", \"memory\": \"2GB\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "yaml_file = TEMP_DIR / \"sample_config.yaml\"\n",
    "with open(yaml_file, \"w\") as f:\n",
    "    yaml.dump(sample_config, f)\n",
    "\n",
    "print(f\"âœ“ Created sample YAML: {yaml_file}\")\n",
    "print(f\"  File size: {yaml_file.stat().st_size} bytes\")\n",
    "print(\"\\nSample structure:\")\n",
    "pprint(sample_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Section 3: Analyze Structure\n",
    "\n",
    "Understand the nested structure before transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(yaml_file) as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "analyzer = StructureAnalyzer(max_depth=4)\n",
    "analysis = analyzer.analyze(data)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STRUCTURE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "analyzer.print_summary(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Section 4: Generate Relational Tables\n",
    "\n",
    "Convert nested YAML to normalized tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_gen = TableGenerator(max_depth=None)  # Full flattening\n",
    "tables = table_gen.generate_tables(data, root_table_name=\"CONFIG\", source_file=yaml_file)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GENERATED TABLES\")\n",
    "print(\"=\" * 70)\n",
    "table_gen.print_summary()\n",
    "\n",
    "print(\"\\nTable Schemas:\")\n",
    "for table_name, df in tables.items():\n",
    "    print(f\"\\nğŸ“Š {table_name}\")\n",
    "    print(f\"   Rows: {len(df)}, Columns: {len(df.columns)}\")\n",
    "    print(f\"   Schema: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Section 5: DuckDB as Middle Layer\n",
    "\n",
    "Load tables into DuckDB for fast querying and transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: File-based DuckDB\n",
    "db_path = TEMP_DIR / \"config.duckdb\"\n",
    "loader = DuckDBLoader(db_path)\n",
    "loader.connect()\n",
    "loader.load_tables(tables, if_exists=\"replace\", create_indexes=True)\n",
    "loader.print_summary()\n",
    "\n",
    "print(f\"\\nâœ“ Database file: {db_path}\")\n",
    "if db_path.exists():\n",
    "    print(f\"  File size: {db_path.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Section 6: DuckDB Advanced Queries\n",
    "\n",
    "Demonstrate DuckDB's powerful SQL capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DUCKDB QUERY EXAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Example 1: List all tables\n",
    "print(\"\\n1ï¸âƒ£ Tables in database:\")\n",
    "tables_list = loader.list_tables()\n",
    "for table in tables_list:\n",
    "    print(f\"   - {table}\")\n",
    "\n",
    "# Example 2: Complex aggregation query\n",
    "if \"CLUSTERS\" in tables_list:\n",
    "    print(\"\\n2ï¸âƒ£ Cluster statistics:\")\n",
    "    result = loader.query('SELECT environment, COUNT(*) as count FROM \"CLUSTERS\" GROUP BY environment')\n",
    "    print(result.to_string(index=False))\n",
    "\n",
    "# Example 3: Schema inspection\n",
    "print(\"\\n3ï¸âƒ£ SAMPLE_CONFIG table schema:\")\n",
    "schema = loader.get_table_info(\"SAMPLE_CONFIG\")\n",
    "print(schema.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Section 7: Generate DDL for Multiple Databases\n",
    "\n",
    "Use DuckDB to generate schemas for different database systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate DDL for Snowflake\n",
    "ddl_gen_sf = DDLGenerator(dialect=\"snowflake\")\n",
    "ddl_snowflake = ddl_gen_sf.generate_ddl(tables, table_gen.relationships)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SQL DDL - SNOWFLAKE\")\n",
    "print(\"=\" * 70)\n",
    "for table_name, sql in list(ddl_snowflake.items())[:2]:  # Show first 2 tables\n",
    "    print(f\"\\n-- Table: {table_name}\")\n",
    "    print(sql[:200] + \"...\" if len(sql) > 200 else sql)\n",
    "\n",
    "# Generate DDL for PostgreSQL\n",
    "ddl_gen_pg = DDLGenerator(dialect=\"postgresql\")\n",
    "ddl_postgres = ddl_gen_pg.generate_ddl(tables, table_gen.relationships)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SQL DDL - POSTGRESQL\")\n",
    "print(\"=\" * 70)\n",
    "for table_name, sql in list(ddl_postgres.items())[:2]:  # Show first 2 tables\n",
    "    print(f\"\\n-- Table: {table_name}\")\n",
    "    print(sql[:200] + \"...\" if len(sql) > 200 else sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Section 8: In-Memory DuckDB\n",
    "\n",
    "Demonstrate DuckDB's in-memory mode for ephemeral data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create in-memory database (no file)\n",
    "print(\"Creating in-memory DuckDB database...\\n\")\n",
    "memory_loader = DuckDBLoader(db_path=None)  # None = in-memory\n",
    "memory_loader.connect()\n",
    "memory_loader.load_tables(tables, if_exists=\"replace\")\n",
    "memory_loader.print_summary()\n",
    "\n",
    "print(\"\\nâœ“ In-memory database is perfect for:\")\n",
    "print(\"  - Temporary data transformations\")\n",
    "print(\"  - Unit testing\")\n",
    "print(\"  - Rapid prototyping\")\n",
    "print(\"  - Avoiding file I/O overhead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Section 9: Performance Comparison\n",
    "\n",
    "Compare DuckDB vs SQLite performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DUCKDB vs SQLITE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison = \"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘ Feature            â•‘ DuckDB          â•‘ SQLite             â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘ Query Speed        â•‘ Very Fast âš¡âš¡âš¡ â•‘ Moderate âš¡âš¡     â•‘\n",
    "â•‘ Type System        â•‘ Rich & Smart    â•‘ Basic              â•‘\n",
    "â•‘ JSON Support       â•‘ Excellent       â•‘ Limited            â•‘\n",
    "â•‘ Memory Efficiency  â•‘ Excellent       â•‘ Good               â•‘\n",
    "â•‘ Aggregations       â•‘ Very Fast       â•‘ Moderate           â•‘\n",
    "â•‘ OLAP Workloads     â•‘ Optimized       â•‘ Not optimized      â•‘\n",
    "â•‘ In-Memory Mode     â•‘ :memory:        â•‘ :memory:           â•‘\n",
    "â•‘ Ease of Use        â•‘ Simple          â•‘ Very Simple        â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "\n",
    "print(comparison)\n",
    "\n",
    "print(\"\\nâœ“ BEST FOR DuckDB:\")\n",
    "print(\"  - Analytical queries on structured data\")\n",
    "print(\"  - JSON/nested data transformation\")\n",
    "print(\"  - Complex aggregations and joins\")\n",
    "print(\"  - High-volume data processing\")\n",
    "print(\"  - In-memory OLAP workloads\")\n",
    "\n",
    "print(\"\\nâœ“ BEST FOR SQLite:\")\n",
    "print(\"  - Simple key-value storage\")\n",
    "print(\"  - Lightweight embedded databases\")\n",
    "print(\"  - Low-latency writes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Section 10: Summary - Why DuckDB is the Better Choice\n",
    "\n",
    "For the YAML Shredder use case, DuckDB is superior as a middle layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"\n",
    "==========================================================================\n",
    "WHY DUCKDB IS THE IDEAL MIDDLE LAYER FOR YAML SHREDDER\n",
    "==========================================================================\n",
    "\n",
    "âœ“ ADVANTAGES FOR DATA TRANSFORMATION:\n",
    "  1. Native JSON support â†’ Perfect for YAML/JSON data\n",
    "  2. Type inference â†’ Automatically detects optimal types\n",
    "  3. Fast aggregations â†’ Analyze nested data quickly\n",
    "  4. Rich functions â†’ JSON extraction, array operations\n",
    "\n",
    "âœ“ ADVANTAGES FOR DDL GENERATION:\n",
    "  1. Better schema inspection â†’ More accurate type mapping\n",
    "  2. Complex queries â†’ Can derive constraints from data\n",
    "  3. Better null handling â†’ Proper NOT NULL detection\n",
    "  4. Index optimization â†’ Suggest key columns\n",
    "\n",
    "âœ“ ADVANTAGES FOR DEPLOYMENT:\n",
    "  1. File-based â†’ Can be version controlled\n",
    "  2. In-memory â†’ No disk overhead during processing\n",
    "  3. Small footprint â†’ Single .duckdb file\n",
    "  4. Cross-platform â†’ Works everywhere Python works\n",
    "\n",
    "âœ“ PYTHON INTEGRATION:\n",
    "  1. Seamless pandas support â†’ df.to_duckdb(), query().df()\n",
    "  2. Simple API â†’ Similar to pandas/SQLite\n",
    "  3. No dependencies â†’ Pure Python (with native library)\n",
    "  4. Type hints â†’ Full Python 3.13+ support\n",
    "\n",
    "==========================================================================\n",
    "RECOMMENDED ARCHITECTURE\n",
    "==========================================================================\n",
    "\n",
    "YAML File\n",
    "    â†“\n",
    "Parse with PyYAML\n",
    "    â†“\n",
    "Table Generation (TableGenerator)\n",
    "    â†“\n",
    "DuckDB â† â† â† [THIS IS YOUR MIDDLE LAYER] â† â† â†\n",
    "    â†“\n",
    "Schema Inference & Analysis\n",
    "    â†“\n",
    "DDL Generation (for Snowflake/PostgreSQL/MySQL/SQLite)\n",
    "    â†“\n",
    "Deploy to Target Database\n",
    "\n",
    "==========================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "print(\"\\nâœ“ Files created in this demo:\")\n",
    "for file in sorted(TEMP_DIR.rglob(\"*\")):\n",
    "    if file.is_file():\n",
    "        rel_path = file.relative_to(TEMP_DIR)\n",
    "        size = file.stat().st_size\n",
    "        print(f\"  {rel_path} ({size:,} bytes)\")\n",
    "\n",
    "print(f\"\\nâœ“ Working directory: {TEMP_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schema-sentinel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
